{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["**Breast Cancer Detection**\n","=\n","\n","***Advanced Methods of Artificial Vision***\n","\n","**Authors:** *Alejandro Araque Robles, Ander Bodegas Díez, Lucía Gonzalez Ratón y Gonzalo Sabando Alonso*"],"metadata":{"id":"OthJAaAdUZ3l"}},{"cell_type":"markdown","source":["# 1. Introduction\n","\n","This is the introduction for the final project."],"metadata":{"id":"wjbVvM5KTVlN"}},{"cell_type":"markdown","source":["# 2. Data Loading\n","\n","First we need to import all the packages that we are going to need for the project."],"metadata":{"id":"vZl9MKC0Thhp"}},{"cell_type":"code","source":["!git config --global user.name \"Lgonrat\"\n","!git config --global user.email \"lgonrat@teleco.upv.es\"\n","!git clone https://github.com/Lgonrat/trabajo-admeav.git\n","!mv *.ipynb trabajo-admeav/"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kwjFHa_u8FYp","executionInfo":{"status":"ok","timestamp":1766421866856,"user_tz":-60,"elapsed":418,"user":{"displayName":"Lucía","userId":"07610550405911545517"}},"outputId":"c9429bfa-f46b-48c9-cfcc-9c0b1a46b269"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["fatal: destination path 'trabajo-admeav' already exists and is not an empty directory.\n","mv: cannot stat '*.ipynb': No such file or directory\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","\n","import os\n","import cv2\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import time"],"metadata":{"id":"DyBxe0tmV00u"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then, we mount the drive to get access to the dataset."],"metadata":{"id":"aYvluWxsXiR4"}},{"cell_type":"code","source":["drive.mount('/content/drive')\n","pathBase = '/content/drive/MyDrive/FinalProjectAdmeav'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6yAbfdTDWDRV","executionInfo":{"status":"ok","timestamp":1765378373041,"user_tz":-60,"elapsed":30741,"user":{"displayName":"Alejandro araque","userId":"18152028971804395702"}},"outputId":"79f21212-54da-4864-f8ba-018326ff005e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["# 3. Data Exploration and Preprocessing\n","\n","In this section, we are going to explore the data structure and perform the adequate operations to obtain a consistent dataset."],"metadata":{"id":"tN5wmKbqTqYJ"}},{"cell_type":"code","source":["# # Benign and malignant paths\n","# pathBenign = os.path.join(pathBase, 'breast_ultrasound', 'benign')\n","# pathMalignant = os.path.join(pathBase, 'breast_ultrasound', 'malignant')\n","\n","# # Loop to read, preprocess each image and save in the new directory\n","# for pathType in [pathBenign, pathMalignant]:\n","#   for name in os.listdir(pathType):\n","\n","#     # Remove '.png' from name\n","#     name = name[:-4]\n","\n","#     # Skip is mask\n","#     if 'mask' in name:\n","#       continue\n","\n","#     # Read image\n","#     img = cv2.imread(\n","#       filename = os.path.join(pathType, f'{name}.png'),\n","#       flags = cv2.IMREAD_UNCHANGED\n","#     )\n","\n","#     # Read base mask\n","#     mas = cv2.imread(\n","#       filename = os.path.join(pathType, f'{name}_mask.png'),\n","#       flags = cv2.IMREAD_UNCHANGED\n","#     )\n","\n","#     # Read additional masks if exist\n","#     for i in range(1, 3):\n","#       if f'{name}_mask_{i}.png' in os.listdir(pathType):\n","\n","#         # Read additional mask\n","#         masAdd = cv2.imread(\n","#           filename = os.path.join(pathType, f'{name}_mask_{i}.png'),\n","#           flags = cv2.IMREAD_UNCHANGED\n","#         )\n","\n","#         # Add additional mask to base mask\n","#         try:\n","#           mas = cv2.bitwise_or(src1 = mas, src2 = masAdd)\n","#         except:\n","#           print(name)\n","#           mas = cv2.bitwise_or(src1 = mas, src2 = masAdd[:, :, 0])\n","\n","#     # Resize image to 512x512 using linear interpolation\n","#     img = cv2.resize(\n","#       src = img,\n","#       dsize = (512, 512),\n","#       interpolation = cv2.INTER_LINEAR\n","#     )\n","\n","#     # Resize mask to 512x512 using nearest neighbor interpolation\n","#     mas = cv2.resize(\n","#       src = mas,\n","#       dsize = (512, 512),\n","#       interpolation = cv2.INTER_NEAREST\n","#     )\n","\n","#     # Save image and mask\n","#     cv2.imwrite(\n","#       os.path.join(pathBase, 'DATASET/IMAGES', f'{name}.png'),\n","#       img\n","#     )\n","#     np.save(\n","#       os.path.join(pathBase, 'DATASET/MASKS', f'{name}.npy'),\n","#       mas\n","#     )"],"metadata":{"id":"CrCiUJ0DX7Z_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to train the segmentation model, we first need to create a PyTorch Dataset."],"metadata":{"id":"-em2LRWiMt_j"}},{"cell_type":"code","source":["class BreastCancerDataset(torch.utils.data.Dataset):\n","  def __init__(self, dir: str, tra = None):\n","    super(BreastCancerDataset, self).__init__()\n","    self.tra = tra\n","    self.imgDir = os.path.join(dir, 'IMAGES')\n","    self.masDir = os.path.join(dir, 'MASKS')\n","    self.imgLis = os.listdir(self.imgDir)\n","    self.masLis = os.listdir(self.masDir)\n","\n","  def __getitem__(self, idx):\n","    img = cv2.imread(\n","      filename = os.path.join(self.imgDir, self.imgLis[idx]),\n","      flags = cv2.IMREAD_UNCHANGED\n","    )\n","    img = torch.from_numpy(img).to(dtype = torch.float32)\n","    img = img.permute(2, 0, 1)\n","    mas = np.load(file = os.path.join(self.masDir, self.masLis[idx]))\n","    mas = torch.from_numpy(mas / 255.).to(dtype = torch.long)\n","    if self.tra:\n","      img = self.tra(img)\n","    return img, mas\n","\n","  def __len__(self):\n","    return len(self.imgLis)"],"metadata":{"id":"DCb4LDd8M1mJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 4. Model Selection and Desing\n","\n","We're going to design a simple UNet architecture, with a skip-connected (optional) encoder and decoder."],"metadata":{"id":"AgHkcygPTyYZ"}},{"cell_type":"code","source":["def convBlock(inChannels: int, outChannels: int, k: int):\n","  return torch.nn.Sequential(\n","    torch.nn.Conv2d(\n","      in_channels = inChannels,\n","      out_channels = outChannels,\n","      kernel_size = (k, k),\n","      stride = 1,\n","      padding = 'same'\n","    ),\n","    torch.nn.BatchNorm2d(outChannels),\n","    torch.nn.ReLU(),\n","    torch.nn.Conv2d(\n","      in_channels = outChannels,\n","      out_channels = outChannels,\n","      kernel_size = (3, 3),\n","      stride = 1,\n","      padding = 'same'\n","    ),\n","    torch.nn.BatchNorm2d(outChannels),\n","    torch.nn.ReLU()\n","  )\n","\n","#------------------------------------------------------------------------------#\n","\n","def up(inChannels: int, outChannels: int, k: int):\n","  return torch.nn.Sequential(\n","    torch.nn.ConvTranspose2d(\n","      in_channels = inChannels,\n","      out_channels = outChannels,\n","      kernel_size = (k, k),\n","      stride = k\n","    )\n","  )\n","\n","#------------------------------------------------------------------------------#\n","\n","class Encoder(torch.nn.Module):\n","  def __init__(\n","    self,\n","    inChannels: int = 1,\n","    initChannels: int = 16,\n","    depthLevels: int = 4\n","  ):\n","    super(Encoder, self).__init__()\n","    self.depthLevels = depthLevels\n","\n","    # Initialize list of modules\n","    self.layers = torch.nn.ModuleList()\n","\n","    # Initialize out channels\n","    outChannels = initChannels\n","\n","    # Loop to generate all layers\n","    for i in range(depthLevels):\n","\n","      # Convolutional block\n","      self.layers.append(\n","        convBlock(\n","          inChannels = inChannels,\n","          outChannels = outChannels,\n","          k = 3\n","        )\n","      )\n","\n","      # Max pooling\n","      self.layers.append(\n","        torch.nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n","      )\n","\n","      # Update in and out channels\n","      inChannels = outChannels\n","      outChannels = outChannels * 2\n","\n","    # Bottle neck (last layer of the encoder)\n","    self.layers.append(\n","      convBlock(\n","        inChannels = inChannels,\n","        outChannels = outChannels,\n","        k = 3\n","      )\n","    )\n","\n","    # Save feature dimensions\n","    self.featureDim = outChannels\n","\n","  def forward(self, x: torch.tensor):\n","    features = list()\n","\n","    # Forward loop\n","    for i in range(self.depthLevels):\n","      x = self.layers[2 * i](x)\n","      features.append(x)\n","      x = self.layers[2 * i + 1](x)\n","    x = torch.nn.Dropout(0.2)(self.layers[-1](x))\n","    features.append(x)\n","    return features\n","\n","#------------------------------------------------------------------------------#\n","\n","class Decoder(torch.nn.Module):\n","  def __init__(\n","    self,\n","    inChannels: int,\n","    nClasses: int = 1,\n","    depthLevels: int = 4,\n","    skip: bool = True\n","  ):\n","    super(Decoder, self).__init__()\n","    self.depthLevels = depthLevels\n","    self.skip = skip\n","\n","    # Initialize list of modules\n","    self.layers = torch.nn.ModuleList()\n","\n","    # Initialize out channels\n","    outChannels = inChannels // 2\n","\n","    # Loop to generate all layers\n","    for _ in range(depthLevels):\n","\n","      # Up block\n","      self.layers.append(\n","        up(\n","          inChannels = inChannels,\n","          outChannels = outChannels,\n","          k = 2\n","        )\n","      )\n","\n","      # Convolutional block\n","      self.layers.append(\n","        convBlock(\n","          inChannels = outChannels * 2 if self.skip else outChannels,\n","          outChannels = outChannels,\n","          k = 3\n","        )\n","      )\n","\n","      # Update in and out channels\n","      inChannels = outChannels\n","      outChannels = inChannels // 2\n","\n","    # Final (last layer of the decoder)\n","    self.layers.append(\n","      torch.nn.Conv2d(\n","        in_channels = inChannels,\n","        out_channels = nClasses,\n","        kernel_size = (3, 3),\n","        stride = 1,\n","        padding = 'same'\n","      )\n","    )\n","\n","  def forward(self, features: list):\n","\n","    # Get bottleneck features\n","    x = features[-1]\n","\n","    # Forward loop with or without skip-connections\n","    for i in range(self.depthLevels):\n","      x = self.layers[2 * i](x)\n","      if self.skip:\n","        x = torch.cat([x, features[-(i + 2)]], dim = 1)\n","      x = self.layers[2 * i + 1](x)\n","    return self.layers[-1](x)\n","\n","#-----------------------------------------------------------------------------#\n","\n","class UNet(torch.nn.Module):\n","  def __init__(\n","    self,\n","    inChannels: int = 3,\n","    nClasses: int = 1,\n","    initChannels: int = 16,\n","    depthLevels: int = 4,\n","    skip: bool = True\n","  ):\n","    super(UNet, self).__init__()\n","\n","    # Encoder\n","    self.encoder = Encoder(\n","      inChannels = inChannels,\n","      initChannels = initChannels,\n","      depthLevels = depthLevels\n","    )\n","\n","    # Decoder\n","    self.decoder = Decoder(\n","      inChannels = self.encoder.featureDim,\n","      nClasses = nClasses,\n","      depthLevels = depthLevels,\n","      skip = skip\n","    )\n","\n","  def forward(self, x: torch.tensor):\n","    features = self.encoder(x)\n","    return self.decoder(features)"],"metadata":{"id":"8bjarNPdMpZl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 5. Training and Validation\n","\n","First we define the Dice loss function."],"metadata":{"id":"PGQbpJLDT4v6"}},{"cell_type":"code","source":["def diceLoss(preds: torch.Tensor, targs: torch.Tensor):\n","\n","  # Compute probabilities with a sigmoid function\n","  preds = torch.sigmoid(preds[:, 0])\n","\n","  # Compute intersection and union\n","  inter = (preds * targs).sum()\n","  union = preds.sum() + targs.sum()\n","\n","  # Compute dice and return 1 - dice\n","  dice = (2 * inter + 1e-6) / (union + 1e-6)\n","  return 1 - dice"],"metadata":{"id":"gw_0yjfOQA0K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we create a training function."],"metadata":{"id":"PkUioz1-SI7p"}},{"cell_type":"code","source":["def train(\n","  model: torch.nn.Module,\n","  optimizer,\n","  criterion,\n","  epochs: int,\n","  dlT: torch.utils.data.DataLoader,\n","  dlV: torch.utils.data.DataLoader,\n","  dev: torch.DeviceObjType\n","):\n","  # Number of steps for the training and validation dataset\n","  stepsT = len(dlT)\n","  stepsV = len(dlV)\n","\n","  # Logs dataframe\n","  logs = {\n","    'epoch': list(),\n","    'time': list(),\n","    'lossT': list(),\n","    'lossV': list()\n","  }\n","\n","  # Move model to device\n","  model.to(device = dev)\n","\n","  # Traing loop\n","  for epoch in range(epochs):\n","\n","    # Initialize time\n","    t0 = time.time()\n","\n","    # Update logs\n","    logs['epoch'].append(epoch)\n","    logs['lossT'].append(0)\n","    logs['lossV'].append(0)\n","\n","    # Set model in training mode\n","    model.train()\n","\n","    # Training batch loop\n","    for batch in dlT:\n","\n","      # Read batch and move to device\n","      img, mas = batch\n","      img = img.to(device = dev)\n","      mas = mas.to(device = dev)\n","\n","      # Reset gradient\n","      optimizer.zero_grad()\n","\n","      # Forward pass\n","      preds = model(img)\n","\n","      # Compute loss\n","      loss = criterion(preds, mas)\n","\n","      # Backward pass\n","      loss.backward()\n","      optimizer.step()\n","\n","      # Update logs\n","      logs['lossT'][-1] += loss.item()\n","\n","    # Set model in evaluation mode\n","    model.eval()\n","\n","    # Disable gradient calculations\n","    with torch.no_grad():\n","\n","      # Validation batch loop\n","      for batch in dlV:\n","\n","        # Read batch and move to device\n","        img, mas = batch\n","        img = img.to(device = dev)\n","        mas = mas.to(device = dev)\n","\n","        # Forward pass\n","        preds = model(img)\n","\n","        # Compute loss\n","        loss = criterion(preds, mas)\n","\n","        # Update logs\n","        logs['lossV'][-1] += loss.item()\n","\n","      # Update logs based on the number of steps\n","      logs['lossT'][-1] /= stepsT\n","      logs['lossV'][-1] /= stepsV\n","\n","    # Calculate time\n","    t1 = time.time()\n","    t = (t1 - t0) / 60.0\n","    logs['time'].append(t)\n","\n","    # Print progress\n","    print((\n","      f'Epoch [{epoch} / {epochs}] | '\n","      f'Time: {logs['time'][-1]: .2f} | '\n","      f'T. Loss: {logs['lossT'][-1]: .2f} | '\n","      f'V. Loss: {logs['lossV'][-1]: .2f} | '\n","    ))\n","\n","  return logs"],"metadata":{"id":"4PMovpiuSFZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Then we can create the dataset, dataloaders, initialize the model and start training."],"metadata":{"id":"dWRXn3I6V5TT"}},{"cell_type":"code","source":["# Check device\n","dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Device: {dev}')\n","\n","# Model initialization\n","model = UNet(inChannels = 3, nClasses = 1, initChannels = 16, depthLevels = 4)\n","\n","# Base dataset\n","ds = BreastCancerDataset(dir = os.path.join(pathBase, 'DATASET'))\n","\n","# Set seed for reproducibility\n","torch.manual_seed(42)\n","\n","# Train (70%), validation (15%) and test (15%)\n","n = len(ds)\n","nTrain = int(0.7 * n)\n","nVal = int(0.15 * n)\n","nTest = n - nTrain - nVal\n","\n","# Split\n","dsTrain, dsVal, dsTest = torch.utils.data.random_split(\n","  ds,\n","  [nTrain, nVal, nTest]\n",")\n","\n","# Dataloaders\n","dlT = torch.utils.data.DataLoader(\n","  dataset = dsTrain,\n","  batch_size = 16,\n","  shuffle = True\n",")\n","dlV = torch.utils.data.DataLoader(\n","  dataset = dsVal,\n","  batch_size = 16,\n","  shuffle = False\n",")\n","\n","# Define criterion and optimizer\n","criterion = diceLoss\n","optimizer = torch.optim.AdamW(\n","  params = model.parameters(),\n","  lr = 1e-3,\n","  weight_decay = 1e-4\n",")\n","\n","# Train\n","logs = train(\n","  model = model,\n","  optimizer = optimizer,\n","  criterion = criterion,\n","  epochs = 30,\n","  dlT = dlT,\n","  dlV = dlV,\n","  dev = dev\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":349},"id":"09Ci52qGV-w1","executionInfo":{"status":"error","timestamp":1765378373958,"user_tz":-60,"elapsed":839,"user":{"displayName":"Alejandro araque","userId":"18152028971804395702"}},"outputId":"56b4b0a4-4b97-4751-98cd-60c52295b9b0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Device: cuda\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/content/drive/MyDrive/FinalProjectAdmeav/DATASET/IMAGES'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-643470761.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Base dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBreastCancerDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpathBase\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DATASET'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Set seed for reproducibility\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-2773279551.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, dir, tra)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgDir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'IMAGES'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasDir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'MASKS'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgLis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasLis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmasDir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/drive/MyDrive/FinalProjectAdmeav/DATASET/IMAGES'"]}]},{"cell_type":"markdown","source":["Plot training process."],"metadata":{"id":"fvGYoBfhkLWj"}},{"cell_type":"code","source":["# Plot training process\n","epochs = len(logs['lossT'])\n","plt.style.use('ggplot')\n","plt.plot(\n","  range(1, epochs + 1),\n","  logs['lossT'],\n","  label = 'Train Loss',\n","  color = 'red',\n","  linestyle = '-'\n",")\n","plt.plot(\n","  range(1, epochs + 1),\n","  logs['lossV'],\n","  label = 'Val Loss',\n","  color = 'red',\n","  linestyle = '--'\n",")\n","plt.title('Training Process')\n","plt.xlabel('Epoch')\n","plt.ylabel('Loss')\n","plt.ylim(0, 1)\n","plt.legend()"],"metadata":{"id":"xud1_3cEkK8K"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# 6. Model Evaluation and Metrics\n","\n","Here we evaluate and measure the model"],"metadata":{"id":"9FvbVbOOT_tI"}},{"cell_type":"markdown","source":["# 7. Discussion and Conclusion\n","\n","Here we discuss the results."],"metadata":{"id":"EHelfSruUHQr"}}]}