{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**3. Tumour Segmentation**\n",
        "=\n",
        "\n",
        "***Advanced Methods of Artificial Vision - Final Project***\n",
        "\n",
        "**Authors:** *Alejandro Araque Robles, Ander Bodegas Díez, Lucía Gonzalez Ratón y Gonzalo Sabando Alonso*"
      ],
      "metadata": {
        "id": "OthJAaAdUZ3l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.1. Data Loading\n",
        "First we need to import all the packages that we are going to need for the project."
      ],
      "metadata": {
        "id": "vZl9MKC0Thhp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from PIL import Image\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torchvision\n",
        "import time\n",
        "import random\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "DyBxe0tmV00u"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then, we mount the drive to get access to the dataset."
      ],
      "metadata": {
        "id": "aYvluWxsXiR4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "pathRoot = '/content/drive/MyDrive/Colab Notebooks/Admeav/FinalProjectAdmeav'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yAbfdTDWDRV",
        "outputId": "f270e602-0315-4a70-ed94-7d2ca0bea1d0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.2. PyTorch Dataset\n",
        "\n",
        "In the last section, we preprocessed and structured the data as follows:\n",
        "- DATASET\n",
        "  - TRAIN\n",
        "    - IMAGES\n",
        "    - MASKS\n",
        "  - VALIDATION\n",
        "    - IMAGES\n",
        "    - MASKS\n",
        "  - TEST\n",
        "    - IMAGES\n",
        "    - MASKS\n",
        "\n",
        "In order to train, validate and test a segmentation model we first need to create a PyTorch dataset to read each image and its mask."
      ],
      "metadata": {
        "id": "tN5wmKbqTqYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class BreastCancerDataset(torch.utils.data.Dataset):\n",
        "  def __init__(self, fol: str, tra = None):\n",
        "    super(BreastCancerDataset, self).__init__()\n",
        "    self.imgFol = os.path.join(fol, 'IMAGES')\n",
        "    self.masFol = os.path.join(fol, 'MASKS')\n",
        "    self.tra = tra\n",
        "    self.imgLis = os.listdir(self.imgFol)\n",
        "    self.masLis = os.listdir(self.masFol)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "\n",
        "    # Read image\n",
        "    img = cv2.imread(\n",
        "      filename = os.path.join(self.imgFol, self.imgLis[idx]),\n",
        "      flags = cv2.IMREAD_UNCHANGED\n",
        "    )\n",
        "    img = Image.fromarray(img)\n",
        "\n",
        "    # Read mask\n",
        "    mas = cv2.imread(\n",
        "      filename = os.path.join(self.masFol, self.masLis[idx]),\n",
        "      flags = cv2.IMREAD_UNCHANGED\n",
        "    )\n",
        "    mas = torch.from_numpy(mas / 255).to(dtype = torch.long)\n",
        "\n",
        "    # Apply transformation to image if given\n",
        "    if self.tra:\n",
        "      img = self.tra(img)\n",
        "\n",
        "    return img, mas\n",
        "\n",
        "  def __len__(self):\n",
        "      return len(self.imgLis)"
      ],
      "metadata": {
        "id": "Lm6aoxkHiVyi"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.3. Model Architecture\n",
        "\n",
        "Now we need to define the segmentation models."
      ],
      "metadata": {
        "id": "WslH5ajijqmE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def convBlock(inChannels: int, outChannels: int, k: int):\n",
        "  return torch.nn.Sequential(\n",
        "    torch.nn.Conv2d(\n",
        "      in_channels = inChannels,\n",
        "      out_channels = outChannels,\n",
        "      kernel_size = (k, k),\n",
        "      stride = 1,\n",
        "      padding = 'same'\n",
        "    ),\n",
        "    torch.nn.BatchNorm2d(outChannels),\n",
        "    torch.nn.ReLU(),\n",
        "    torch.nn.Conv2d(\n",
        "      in_channels = outChannels,\n",
        "      out_channels = outChannels,\n",
        "      kernel_size = (3, 3),\n",
        "      stride = 1,\n",
        "      padding = 'same'\n",
        "    ),\n",
        "    torch.nn.BatchNorm2d(outChannels),\n",
        "    torch.nn.ReLU()\n",
        "  )\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "def up(inChannels: int, outChannels: int, k: int):\n",
        "  return torch.nn.Sequential(\n",
        "    torch.nn.ConvTranspose2d(\n",
        "      in_channels = inChannels,\n",
        "      out_channels = outChannels,\n",
        "      kernel_size = (k, k),\n",
        "      stride = k\n",
        "    )\n",
        "  )\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "class Encoder(torch.nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    inChannels: int = 1,\n",
        "    initChannels: int = 16,\n",
        "    depthLevels: int = 4\n",
        "  ):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.depthLevels = depthLevels\n",
        "\n",
        "    # Initialize list of modules\n",
        "    self.layers = torch.nn.ModuleList()\n",
        "\n",
        "    # Initialize out channels\n",
        "    outChannels = initChannels\n",
        "\n",
        "    # Loop to generate all layers\n",
        "    for i in range(depthLevels):\n",
        "\n",
        "      # Convolutional block\n",
        "      self.layers.append(\n",
        "        convBlock(\n",
        "          inChannels = inChannels,\n",
        "          outChannels = outChannels,\n",
        "          k = 3\n",
        "        )\n",
        "      )\n",
        "\n",
        "      # Max pooling\n",
        "      self.layers.append(\n",
        "        torch.nn.MaxPool2d(kernel_size = (2, 2), stride = 2)\n",
        "      )\n",
        "\n",
        "      # Update in and out channels\n",
        "      inChannels = outChannels\n",
        "      outChannels = outChannels * 2\n",
        "\n",
        "    # Bottle neck (last layer of the encoder)\n",
        "    self.layers.append(\n",
        "      convBlock(\n",
        "        inChannels = inChannels,\n",
        "        outChannels = outChannels,\n",
        "        k = 3\n",
        "      )\n",
        "    )\n",
        "\n",
        "    # Save feature dimensions\n",
        "    self.featureDim = outChannels\n",
        "\n",
        "  def forward(self, x: torch.tensor):\n",
        "    features = list()\n",
        "\n",
        "    # Forward loop\n",
        "    for i in range(self.depthLevels):\n",
        "      x = self.layers[2 * i](x)\n",
        "      features.append(x)\n",
        "      x = self.layers[2 * i + 1](x)\n",
        "    x = torch.nn.Dropout(0.2)(self.layers[-1](x))\n",
        "    features.append(x)\n",
        "    return features\n",
        "\n",
        "#------------------------------------------------------------------------------#\n",
        "\n",
        "class Decoder(torch.nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    inChannels: int,\n",
        "    nClasses: int = 1,\n",
        "    depthLevels: int = 4,\n",
        "    skipConnections: bool = True\n",
        "  ):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.depthLevels = depthLevels\n",
        "    self.skipConnections = skipConnections\n",
        "\n",
        "    # Initialize list of modules\n",
        "    self.layers = torch.nn.ModuleList()\n",
        "\n",
        "    # Initialize out channels\n",
        "    outChannels = inChannels // 2\n",
        "\n",
        "    # Loop to generate all layers\n",
        "    for _ in range(depthLevels):\n",
        "\n",
        "      # Up block\n",
        "      self.layers.append(\n",
        "        up(\n",
        "          inChannels = inChannels,\n",
        "          outChannels = outChannels,\n",
        "          k = 2\n",
        "        )\n",
        "      )\n",
        "\n",
        "      # Convolutional block\n",
        "      self.layers.append(\n",
        "        convBlock(\n",
        "          inChannels = outChannels * 2 if self.skipConnections else outChannels,\n",
        "          outChannels = outChannels,\n",
        "          k = 3\n",
        "        )\n",
        "      )\n",
        "\n",
        "      # Update in and out channels\n",
        "      inChannels = outChannels\n",
        "      outChannels = inChannels // 2\n",
        "\n",
        "    # Final (last layer of the decoder)\n",
        "    self.layers.append(\n",
        "      torch.nn.Conv2d(\n",
        "        in_channels = inChannels,\n",
        "        out_channels = nClasses,\n",
        "        kernel_size = (3, 3),\n",
        "        stride = 1,\n",
        "        padding = 'same'\n",
        "      )\n",
        "    )\n",
        "\n",
        "  def forward(self, features: list):\n",
        "\n",
        "    # Get bottleneck features\n",
        "    x = features[-1]\n",
        "\n",
        "    # Forward loop with or without skip-connections\n",
        "    for i in range(self.depthLevels):\n",
        "      x = self.layers[2 * i](x)\n",
        "      if self.skipConnections:\n",
        "        x = torch.cat([x, features[-(i + 2)]], dim = 1)\n",
        "      x = self.layers[2 * i + 1](x)\n",
        "    return self.layers[-1](x)\n",
        "\n",
        "#-----------------------------------------------------------------------------#\n",
        "\n",
        "class UNet(torch.nn.Module):\n",
        "  def __init__(\n",
        "    self,\n",
        "    inChannels: int = 3,\n",
        "    nClasses: int = 1,\n",
        "    initChannels: int = 16,\n",
        "    depthLevels: int = 4,\n",
        "    skipConnections: bool = True\n",
        "  ):\n",
        "    super(UNet, self).__init__()\n",
        "\n",
        "    # Encoder\n",
        "    self.encoder = Encoder(\n",
        "      inChannels = inChannels,\n",
        "      initChannels = initChannels,\n",
        "      depthLevels = depthLevels\n",
        "    )\n",
        "\n",
        "    # Decoder\n",
        "    self.decoder = Decoder(\n",
        "      inChannels = self.encoder.featureDim,\n",
        "      nClasses = nClasses,\n",
        "      depthLevels = depthLevels,\n",
        "      skipConnections = skipConnections\n",
        "    )\n",
        "\n",
        "  def forward(self, x: torch.tensor):\n",
        "    features = self.encoder(x)\n",
        "    return self.decoder(features)\n"
      ],
      "metadata": {
        "id": "26UjZKAE9Sra"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3.4. Training model\n"
      ],
      "metadata": {
        "id": "ifrSHlwW96ic"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def diceLoss(preds: torch.Tensor, targs: torch.Tensor):\n",
        "\n",
        "  # Compute probabilities with a sigmoid function\n",
        "  preds = torch.sigmoid(preds[:, 0])\n",
        "\n",
        "  # Compute intersection and union\n",
        "  inter = (preds * targs).sum()\n",
        "  union = preds.sum() + targs.sum()\n",
        "\n",
        "  # Compute dice and return 1 - dice\n",
        "  dice = (2 * inter + 1e-6) / (union + 1e-6)\n",
        "  return 1 - dice"
      ],
      "metadata": {
        "id": "VzFZu2Pz-eER"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(\n",
        "  model: torch.nn.Module,\n",
        "  optimizer,\n",
        "  criterion,\n",
        "  epochs: int,\n",
        "  dlT: torch.utils.data.DataLoader,\n",
        "  dlV: torch.utils.data.DataLoader,\n",
        "  dev: torch.DeviceObjType\n",
        "):\n",
        "\n",
        "  # Number of steps for the training and validation dataset\n",
        "  stepsT = len(dlT)\n",
        "  stepsV = len(dlV)\n",
        "\n",
        "  # Logs dataframe\n",
        "  logs = {\n",
        "    'epoch': list(),\n",
        "    'time': list(),\n",
        "    'lossT': list(),\n",
        "    'lossV': list()\n",
        "  }\n",
        "\n",
        "  # Move model to device\n",
        "  model.to(device = dev)\n",
        "\n",
        "  # Traing loop\n",
        "  for epoch in range(epochs):\n",
        "\n",
        "    # Initialize time\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Update logs\n",
        "    logs['epoch'].append(epoch)\n",
        "    logs['lossT'].append(0)\n",
        "    logs['lossV'].append(0)\n",
        "\n",
        "    # Set model in training mode\n",
        "    model.train()\n",
        "\n",
        "    # Training batch loop\n",
        "    for batch in dlT:\n",
        "\n",
        "      # Read batch and move to device\n",
        "      img, mas = batch\n",
        "      img = img.to(device = dev)\n",
        "      mas = mas.to(device = dev)\n",
        "\n",
        "      # Reset gradient\n",
        "      optimizer.zero_grad()\n",
        "\n",
        "      # Forward pass\n",
        "      preds = model(img)\n",
        "\n",
        "      # Compute loss\n",
        "      loss = criterion(preds, mas)\n",
        "\n",
        "      # Backward pass\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      # Update logs\n",
        "      logs['lossT'][-1] += loss.item()\n",
        "\n",
        "    # Set model in evaluation mode\n",
        "    model.eval()\n",
        "\n",
        "    # Disable gradient calculations\n",
        "    with torch.no_grad():\n",
        "\n",
        "      # Validation batch loop\n",
        "      for batch in dlV:\n",
        "\n",
        "        # Read batch and move to device\n",
        "        img, mas = batch\n",
        "        img = img.to(device = dev)\n",
        "        mas = mas.to(device = dev)\n",
        "\n",
        "        # Forward pass\n",
        "        preds = model(img)\n",
        "\n",
        "        # Compute loss\n",
        "        loss = criterion(preds, mas)\n",
        "\n",
        "        # Update logs\n",
        "        logs['lossV'][-1] += loss.item()\n",
        "\n",
        "      # Update logs based on the number of steps\n",
        "      logs['lossT'][-1] /= stepsT\n",
        "      logs['lossV'][-1] /= stepsV\n",
        "\n",
        "    # Calculate time\n",
        "    t1 = time.time()\n",
        "    t = (t1 - t0) / 60.0\n",
        "    logs['time'].append(t)\n",
        "\n",
        "    # Print progress\n",
        "    print((\n",
        "      f'Epoch [{epoch} / {epochs}] | '\n",
        "      f'Time: {logs['time'][-1]: .2f} | '\n",
        "      f'T. Loss: {logs['lossT'][-1]: .2f} | '\n",
        "      f'V. Loss: {logs['lossV'][-1]: .2f} | '\n",
        "    ))\n",
        "\n",
        "  return logs"
      ],
      "metadata": {
        "id": "rTjlx5qd-Fx1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check device\n",
        "dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Device: {dev}')\n",
        "\n",
        "# Model initialization\n",
        "model = UNet(inChannels = 3, nClasses = 1, initChannels = 16, depthLevels = 4)\n",
        "\n",
        "# Base dataset\n",
        "dsT = BreastCancerDataset(\n",
        "  fol = os.path.join(pathRoot, 'DATASET/TRAIN'),\n",
        "  tra = torchvision.transforms.ToTensor()\n",
        ")\n",
        "dsV = BreastCancerDataset(\n",
        "  fol = os.path.join(pathRoot, 'DATASET/VALIDATION'),\n",
        "  tra = torchvision.transforms.ToTensor()\n",
        ")\n",
        "\n",
        "# Set seed for reproducibility\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Dataloaders\n",
        "dlT = torch.utils.data.DataLoader(\n",
        "  dataset = dsT,\n",
        "  batch_size = 16,\n",
        "  shuffle = True\n",
        ")\n",
        "dlV = torch.utils.data.DataLoader(\n",
        "  dataset = dsV,\n",
        "  batch_size = 16,\n",
        "  shuffle = False\n",
        ")\n",
        "\n",
        "# Define criterion and optimizer\n",
        "criterion = diceLoss\n",
        "optimizer = torch.optim.AdamW(\n",
        "  params = model.parameters(),\n",
        "  lr = 1e-3,\n",
        "  weight_decay = 1e-4\n",
        ")\n",
        "\n",
        "# Train\n",
        "logs = train(\n",
        "  model = model,\n",
        "  optimizer = optimizer,\n",
        "  criterion = criterion,\n",
        "  epochs = 30,\n",
        "  dlT = dlT,\n",
        "  dlV = dlV,\n",
        "  dev = dev\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CklRG1u9-Y_X",
        "outputId": "3adb07e9-c4d9-451b-a5c3-7dec756d2ee8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "Epoch [0 / 30] | Time:  0.54 | T. Loss:  0.82 | V. Loss:  0.81 | \n",
            "Epoch [1 / 30] | Time:  0.53 | T. Loss:  0.77 | V. Loss:  0.79 | \n",
            "Epoch [2 / 30] | Time:  0.53 | T. Loss:  0.73 | V. Loss:  0.73 | \n",
            "Epoch [3 / 30] | Time:  0.53 | T. Loss:  0.71 | V. Loss:  0.74 | \n",
            "Epoch [4 / 30] | Time:  0.53 | T. Loss:  0.70 | V. Loss:  0.85 | \n",
            "Epoch [5 / 30] | Time:  0.59 | T. Loss:  0.70 | V. Loss:  0.70 | \n",
            "Epoch [6 / 30] | Time:  0.53 | T. Loss:  0.69 | V. Loss:  0.71 | \n"
          ]
        }
      ]
    }
  ]
}